{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Generator training: generative models for Darshan trace\n",
    "\n",
    "This is for a demonstration of how to train Trace Generator for Darshan trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_block(torch.nn.Module):\n",
    "    def __init__(self, n, act=torch.nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        self.module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n, n), torch.nn.LeakyReLU(), torch.nn.Linear(n, n),\n",
    "        )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.module(inputs)\n",
    "        return self.act(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, indim, outdim, nh=8, nz=4):\n",
    "        super(VAE, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "\n",
    "        self._enc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, nh),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._dec = torch.nn.Sequential(\n",
    "            torch.nn.Linear(nz, nh),\n",
    "            torch.nn.Linear(nh, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, outdim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # self.fc1 = nn.Linear(self.indim, nh)\n",
    "        self.fc21 = nn.Linear(nh, nz)\n",
    "        self.fc22 = nn.Linear(nh, nz)\n",
    "        # self.fc3 = nn.Linear(nz, nh)\n",
    "        # self.fc4 = nn.Linear(nh, self.outdim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # h1 = F.leaky_relu(self.fc1(x))\n",
    "        h1 = self._enc(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # h3 = F.leaky_relu(self.fc3(z))\n",
    "        # return torch.sigmoid(self.fc4(h3))\n",
    "        return self._dec(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.indim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, indim, outdim, nz=20):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            nn.Linear(32, nz),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(nz, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            nn.Linear(32, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            nn.Linear(64, outdim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, outdim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, indim, outdim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, outdim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/rmse-loss-function/16540/2\n",
    "def RMSELoss(yhat, y):\n",
    "    return torch.sqrt(torch.mean((yhat - y) ** 2) + torch.finfo(torch.float32).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def vae_loss_function(recon_x, x, mu, logvar, dim=20, alpha=1.0):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, dim), reduction=\"sum\")\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, dim), reduction=\"sum\")\n",
    "    # RMSE = torch.sum(RMSELoss(recon_x, x.view(-1, 20)))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + alpha * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"TG training\")\n",
    "parser.add_argument(\n",
    "    \"--batch-size\",\n",
    "    type=int,\n",
    "    default=128,\n",
    "    metavar=\"N\",\n",
    "    help=\"input batch size for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of epochs to train (default: 10)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--log-interval\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    metavar=\"N\",\n",
    "    help=\"how many batches to wait before logging training status\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args([\"--batch-size=32\"])\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "dtype = \"huge\"  #'huge', large', 'medium'\n",
    "DIM = 0  ## 7, 25, 35\n",
    "NSAMPLES = 20\n",
    "restart_model = None\n",
    "expname = None\n",
    "save_model = False\n",
    "use_fg = True\n",
    "modelname = \"net\"\n",
    "MSAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dtype == \"huge\":\n",
    "    nlen, nclass, MDIM = 26, 10, 75 + 7\n",
    "if dtype == \"large\":\n",
    "    nlen, nclass, MDIM = 220, 10, 60 + 7\n",
    "if dtype == \"medium\":\n",
    "    # nlen, nclass = 945, 12\n",
    "    nlen, nclass, MDIM = 417, 7, 25 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DIM > MDIM) or (DIM < 1):\n",
    "    DIM = MDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Parameters\")\n",
    "for pname in [\n",
    "    \"dtype\",\n",
    "    \"MDIM\",\n",
    "    \"DIM\",\n",
    "    \"NSAMPLES\",\n",
    "    \"restart_model\",\n",
    "    \"expname\",\n",
    "    \"save_model\",\n",
    "    \"use_fg\",\n",
    "    \"modelname\",\n",
    "    \"MSAMPLES\",\n",
    "]:\n",
    "    print(\"%s: %r\" % (pname, eval(pname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if expname is None:\n",
    "    now = datetime.now()\n",
    "    expname = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    expname = \"%s-%d\" % (expname, random.randint(1000, 9999))\n",
    "print(\"expname:\", expname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_fg:\n",
    "    x = np.load(\"flow_train_x_%s_%d_DIM%d_flow.npy\" % (dtype, nlen, MDIM))\n",
    "    y = np.load(\"flow_train_y_%s_%d_DIM%d_flow.npy\" % (dtype, nlen, MDIM))\n",
    "    x = x[:, :DIM]\n",
    "\n",
    "    lb = np.load(\"flow_train_lb_%s_%d_DIM%d_flow.npy\" % (dtype, nlen, MDIM))\n",
    "    app = np.load(\"train_classes_%s_%d_%d.npy\" % (dtype, nlen, nclass))\n",
    "    plt.hist(lb, bins=np.arange(0, len(app) + 1))\n",
    "else:\n",
    "    x = np.load(\"train_x_%s_%d.npy\" % (dtype, nlen))\n",
    "    y = np.load(\"train_y_%s_%d.npy\" % (dtype, nlen))\n",
    "    x = x[:, :DIM]\n",
    "\n",
    "    lb = np.load(\"train_lb_%s_%d.npy\" % (dtype, nlen))\n",
    "    app = np.load(\"train_classes_%s_%d_%d.npy\" % (dtype, nlen, nclass))\n",
    "    plt.hist(lb, bins=np.arange(0, len(app) + 1))\n",
    "\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    lb_list = list()\n",
    "\n",
    "    for i in range(nclass):\n",
    "        k = np.where(lb == i)[0]\n",
    "        k = np.random.choice(k, MSAMPLES)\n",
    "\n",
    "        x_list.append(x[k])\n",
    "        y_list.append(y[k])\n",
    "        lb_list.append(lb[k])\n",
    "\n",
    "    x = np.vstack(x_list)\n",
    "    y = np.vstack(y_list)\n",
    "    lb = np.hstack(lb_list)\n",
    "\n",
    "print(x.shape, y.shape, lb.shape, app.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xapp = np.zeros((len(x), len(app)), dtype=np.float32)\n",
    "xapp[np.arange(len(x)), lb] = 1.0\n",
    "xx = np.hstack((x, xapp))\n",
    "\n",
    "xapp.shape, xx.shape, x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.where(xapp == 1.0)[1], bins=np.arange(0, len(app) + 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, lb_train, lb_test = train_test_split(\n",
    "    xx, y, lb, test_size=0.1\n",
    ")\n",
    "print(len(X_train), len(y_train), len(lb_train), len(X_test), len(y_test), len(lb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 4])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(xapp)\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x)\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(y)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression/Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET = torch.nn.Sequential(\n",
    "    torch.nn.Linear(nclass + DIM, 128),\n",
    "    # 32 filters in and out, no max pooling so the shapes can be added\n",
    "    ResNet_block(128),\n",
    "    ResNet_block(128),\n",
    "    torch.nn.Linear(128, 64),\n",
    "    ResNet_block(64),\n",
    "    ResNet_block(64),\n",
    "    torch.nn.Linear(64, 32),\n",
    "    ResNet_block(32),\n",
    "    ResNet_block(32),\n",
    "    torch.nn.Linear(32, 20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {\n",
    "    \"net\": NET,\n",
    "    \"vae\": VAE(nclass + DIM, 20, nh=32, nz=16),\n",
    "    \"ae\": autoencoder(nclass + DIM, 20),\n",
    "    \"gan\": Generator(nclass + DIM, 20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, model in model_list.items():\n",
    "    print(\"Model params:\", k)\n",
    "    print(\"-\" * 20)\n",
    "    num_params = 0\n",
    "    for k, v in model.state_dict().items():\n",
    "        print(\"%20s\\t%20s\\t%10d\" % (k, list(v.shape), v.numel()))\n",
    "        num_params += v.numel()\n",
    "    print(\"-\" * 50)\n",
    "    print(\"%20s\\t%20s\\t%10d\" % (\"Total\", \"\", num_params))\n",
    "    print(\"All (total, MB): %d %g\" % (num_params, num_params * 4 / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_train), torch.tensor(y_train), torch.tensor(lb_train)\n",
    ")\n",
    "testing_data = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_test), torch.tensor(y_test), torch.tensor(lb_test)\n",
    ")\n",
    "full_data = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(np.hstack((x, xapp))), torch.tensor(y), torch.tensor(lb)\n",
    ")\n",
    "\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_data, batch_size=args.batch_size, shuffle=True, drop_last=False, **kwargs\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testing_data, batch_size=args.batch_size, shuffle=True, drop_last=False, **kwargs\n",
    ")\n",
    "full_loader = torch.utils.data.DataLoader(\n",
    "    full_data, batch_size=args.batch_size, shuffle=True, drop_last=False, **kwargs\n",
    ")\n",
    "\n",
    "print(len(train_loader), len(test_loader), len(full_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "model = model_list[modelname].to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2_000, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=200)\n",
    "epoch = 0\n",
    "\n",
    "if modelname == \"gan\":\n",
    "    discriminator = Discriminator(20).to(device)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    scheduler_D = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_D, \"min\", patience=200\n",
    "    )\n",
    "    adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    if restart_model is not None:\n",
    "        break\n",
    "    train_loss_list = list()\n",
    "    for _ in range(100):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, lab, _) in enumerate(train_loader, 1):\n",
    "            data, lab = data.to(device), lab.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if modelname == \"vae\":\n",
    "                recon_batch, mu, logvar = model(data)\n",
    "                loss = vae_loss_function(recon_batch, lab, mu, logvar)\n",
    "            else:\n",
    "                recon_batch = model(data)\n",
    "                # loss = loss_func(recon_batch, lab)\n",
    "                if modelname == \"gan\":\n",
    "                    valid = Variable(\n",
    "                        torch.Tensor(data.size(0), 1).fill_(1.0), requires_grad=False\n",
    "                    )\n",
    "                    fake = Variable(\n",
    "                        torch.Tensor(data.size(0), 1).fill_(0.0), requires_grad=False\n",
    "                    )\n",
    "                    loss = adversarial_loss(discriminator(recon_batch), valid)\n",
    "                else:\n",
    "                    loss = loss_func(recon_batch, lab)\n",
    "\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            if modelname == \"gan\":\n",
    "                ## Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = adversarial_loss(discriminator(lab), valid)\n",
    "                fake_loss = adversarial_loss(discriminator(recon_batch.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.07g}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item() / len(data),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        train_loss_list.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                \"====> Epoch: {} Average loss: {:.04g} LR: {:.03g}\".format(\n",
    "                    epoch,\n",
    "                    train_loss / len(train_loader.dataset),\n",
    "                    optimizer.param_groups[0][\"lr\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"====> Epoch: {} Resampling ...\".format(epoch))\n",
    "            model.eval()\n",
    "            recon_loader = torch.utils.data.DataLoader(\n",
    "                training_data,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                **kwargs\n",
    "            )\n",
    "            err_list = list()\n",
    "            with torch.no_grad():\n",
    "                for i, (data, lab, _) in enumerate(recon_loader):\n",
    "                    data, lab = data.to(device), lab.to(device)\n",
    "                    recon_batch = model(data)\n",
    "                    if modelname == \"vae\":\n",
    "                        recon_batch = recon_batch[0]\n",
    "                    err = torch.sum(\n",
    "                        torch.sqrt(\n",
    "                            F.mse_loss(recon_batch, lab.view(-1, 20), reduction=\"none\")\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    err_list.extend(err.detach().cpu().numpy())\n",
    "\n",
    "                weights = err_list / np.sum(err_list)\n",
    "                sampler = torch.utils.data.WeightedRandomSampler(\n",
    "                    weights, len(weights), replacement=True\n",
    "                )\n",
    "                train_loader = torch.utils.data.DataLoader(\n",
    "                    training_data,\n",
    "                    batch_size=args.batch_size,\n",
    "                    shuffle=False,\n",
    "                    drop_last=False,\n",
    "                    sampler=sampler,\n",
    "                )\n",
    "\n",
    "        scheduler.step(loss)\n",
    "        if modelname == \"gan\":\n",
    "            scheduler_D.step(d_loss)\n",
    "\n",
    "    if (k + 1) % 10 == 0:\n",
    "        plt.figure()\n",
    "        plt.plot(train_loss_list)\n",
    "        plt.title(\"Epoch: %d MSE: %g\" % (epoch, np.mean(train_loss_list)))\n",
    "        plt.show()\n",
    "\n",
    "        fname = \"gmodel_%s_%d_DIM%d_%s_fg%d.torch\" % (\n",
    "            dtype,\n",
    "            nlen,\n",
    "            DIM,\n",
    "            modelname,\n",
    "            use_fg,\n",
    "        )\n",
    "        torch.save(model.state_dict(), fname)\n",
    "        print(\"Model saved:\", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart_model is None:\n",
    "    fname = \"gmodel_%s_%d_DIM%d.%s.torch\" % (dtype, nlen, DIM, expname)\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), fname)\n",
    "        print(\"Model saved:\", fname)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(restart_model))\n",
    "    print(\"Model loaded:\", restart_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "lab_list = list()\n",
    "recon_list = list()\n",
    "err_list = list()\n",
    "with torch.no_grad():\n",
    "    for i, (data, lab, _) in enumerate(test_loader):\n",
    "        data, lab = data.to(device), lab.to(device)\n",
    "        recon_batch = model(data)\n",
    "        if modelname == \"vae\":\n",
    "            recon_batch = recon_batch[0]\n",
    "        mk0 = recon_batch < 0.0\n",
    "        recon_batch[mk0] = 0.0\n",
    "        mk1 = recon_batch > 1.0\n",
    "        recon_batch[mk1] = 1.0\n",
    "\n",
    "        test_loss += loss_func(recon_batch, lab).item()\n",
    "        lab_list.append(lab.detach().cpu().numpy())\n",
    "        recon_list.append(recon_batch.detach().cpu().numpy())\n",
    "        # https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/\n",
    "        err = torch.sqrt(\n",
    "            torch.mean(\n",
    "                F.mse_loss(recon_batch, lab.view(-1, 20), reduction=\"none\"), axis=1\n",
    "            )\n",
    "        )\n",
    "        err_list.extend(err.detach().cpu().numpy())\n",
    "\n",
    "lab_arr = np.concatenate(lab_list)\n",
    "recon_arr = np.concatenate(recon_list)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(\"len(test_loader.dataset)\", len(test_loader.dataset))\n",
    "print(\"====> Test set loss: {:f}\".format(test_loss))\n",
    "print(\"====> Test RMSE loss (min,max):\", np.min(err_list), np.max(err_list))\n",
    "print(\"====> Test RMSE loss (median):\", np.median(err_list))\n",
    "print(\"====> Test RMSE loss (mean,std):\", np.mean(err_list), np.std(err_list))\n",
    "print(\"====> Test RMSE loss (mode):\", stats.mode(err_list)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(lab.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(recon_batch.detach().cpu().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(err_list), len(full_loader.dataset), len(full_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "lab_list = list()\n",
    "recon_list = list()\n",
    "ax_list = list()\n",
    "err_list = list()\n",
    "with torch.no_grad():\n",
    "    for i, (data, lab, ax) in enumerate(full_loader):\n",
    "        data, lab, ax = data.to(device), lab.to(device), ax.to(device)\n",
    "        recon_batch = model(data)\n",
    "        if modelname == \"vae\":\n",
    "            recon_batch = recon_batch[0]\n",
    "        mk0 = recon_batch < 0.0\n",
    "        recon_batch[mk0] = 0.0\n",
    "        mk1 = recon_batch > 1.0\n",
    "        recon_batch[mk1] = 1.0\n",
    "        test_loss += loss_func(recon_batch, lab).item()\n",
    "        lab_list.append(lab.detach().cpu().numpy())\n",
    "        recon_list.append(recon_batch.detach().cpu().numpy())\n",
    "        ax_list.append(ax.detach().cpu().numpy())\n",
    "        # https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/\n",
    "        err = torch.sqrt(\n",
    "            torch.mean(\n",
    "                F.mse_loss(recon_batch, lab.view(-1, 20), reduction=\"none\"), axis=1\n",
    "            )\n",
    "        )\n",
    "        err_list.extend(err.detach().cpu().numpy())\n",
    "\n",
    "lab_arr = np.concatenate(lab_list)\n",
    "recon_arr = np.concatenate(recon_list)\n",
    "ax_arr = np.concatenate(ax_list)\n",
    "print(\"test_loss\", test_loss)\n",
    "\n",
    "test_loss /= len(full_loader.dataset)\n",
    "print(\"len(full_loader.dataset)\", len(full_loader.dataset))\n",
    "print(\"====> Full set loss: {:f}\".format(test_loss))\n",
    "print(\n",
    "    \"====> Full set loss (RMSE): {:f}\".format(\n",
    "        np.sum(err_list) / len(full_loader.dataset)\n",
    "    )\n",
    ")\n",
    "print(\"====> Full RMSE loss (min,max):\", np.min(err_list), np.max(err_list))\n",
    "print(\"====> Full RMSE loss (median):\", np.median(err_list))\n",
    "print(\"====> Full RMSE loss (mean,std):\", np.mean(err_list), np.std(err_list))\n",
    "print(\"====> Full RMSE loss (mode):\", stats.mode(err_list)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(err_list), test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(lab.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(recon_batch.detach().cpu().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lab)):\n",
    "    plt.figure()\n",
    "    plt.plot(lab[i, :].detach().cpu().numpy(), \".-\", label=\"Original\")\n",
    "    plt.plot(recon_batch[i, :].detach().cpu().numpy(), \".-\", label=\"Recon\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(err_list))\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"ID\")\n",
    "plt.plot(err_list)\n",
    "plt.axhline(np.median(err_list), c=\"g\", label=\"median\")\n",
    "plt.axhline(np.mean(err_list), c=\"r\", label=\"mean\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"\")\n",
    "plt.hist(err_list)\n",
    "plt.axvline(np.median(err_list), c=\"g\", label=\"median\")\n",
    "plt.axvline(np.mean(err_list), c=\"r\", label=\"mean\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = np.argsort(err_list)\n",
    "print(len(err_list))\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.array(err_list)[od])\n",
    "plt.axhline(np.median(err_list), c=\"g\", label=\"median\")\n",
    "plt.axhline(np.mean(err_list), c=\"r\", label=\"mean\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(err_list)\n",
    "plt.axvline(np.median(err_list), c=\"g\", label=\"median\")\n",
    "plt.axvline(np.mean(err_list), c=\"r\", label=\"mean\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = np.argsort(err_list)\n",
    "for i in od[:5]:\n",
    "    plt.figure()\n",
    "    plt.plot(lab_arr[i, :], \".-\", label=\"Original\")\n",
    "    plt.plot(recon_arr[i, :], \".-\", label=\"Recon\")\n",
    "    plt.legend()\n",
    "    plt.ylim([-0.2, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = np.argsort(err_list)\n",
    "for i in od[::-1][:5]:\n",
    "    plt.figure()\n",
    "    plt.plot(lab_arr[i, :], \".-\", label=\"Original\")\n",
    "    plt.plot(recon_arr[i, :], \".-\", label=\"Recon\")\n",
    "    plt.legend()\n",
    "    # plt.ylim([-0.2,1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizing_flows import NormalizingFlow\n",
    "from scipy.interpolate import interp1d, make_interp_spline\n",
    "from utils import random_normal_samples\n",
    "\n",
    "# from scipy.signal import savgol_filter\n",
    "# from scipy.interpolate import Rbf\n",
    "# from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "# from scipy.interpolate import interp1d\n",
    "# from scipy.interpolate import make_lsq_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loess(x, y, frac=0.2, it=None, scatter=True):\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    y = y[x.argsort()]  # Sort y according to order of x.\n",
    "    x.sort()  # Sort x in place.\n",
    "    if it is not None:  # Helps if you are getting NaN's in the output.\n",
    "        d = lowess(y, x, frac=frac, it=it)\n",
    "    else:\n",
    "        d = lowess(y, x, frac=frac)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), \"valid\") / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(lab, n=40):\n",
    "    vf1 = np.vectorize(lambda x: max(x, 0.0))\n",
    "    vf2 = np.vectorize(lambda x: min(x, 1.0))\n",
    "\n",
    "    lab = vf1(lab)\n",
    "    lab = vf2(lab)\n",
    "\n",
    "    x = np.linspace(0, 1, 20)\n",
    "    intp = interp1d(x, lab, kind=\"slinear\")\n",
    "\n",
    "    xi = np.linspace(x.min(), x.max(), n)\n",
    "    yi = intp(xi)\n",
    "    # yi = moving_average(yi, k)\n",
    "\n",
    "    # intp2 = make_interp_spline(xi, yi)\n",
    "    # xs = xi\n",
    "    # ys = intp2(xs)\n",
    "\n",
    "    # return (xs, ys)\n",
    "    return (xi, yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(app)):\n",
    "    fig = plt.figure(figsize=[20, 4])\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    for recon, lab, adx in zip(recon_arr, lab_arr, ax_arr):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(lab)\n",
    "            ax1.plot(xs, ys)\n",
    "\n",
    "            xs, ys = smooth(recon)\n",
    "            ax2.plot(xs, ys)\n",
    "    ax1.set_title(\"%s (Original)\" % (app[i]))\n",
    "    ax1.set_xlabel(\"Time (Normalized)\")\n",
    "    ax1.set_ylabel(\"Write I/O Intensity (Normalized)\")\n",
    "    ax2.set_title(\"%s (Generated)\" % (app[i]))\n",
    "    ax2.set_xlabel(\"Time (Normalized)\")\n",
    "    ax2.set_ylabel(\"Write I/O Intensity (Normalized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(app)):\n",
    "    fig = plt.figure(figsize=[20, 4])\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    for recon, lab, adx in zip(recon_arr, lab_arr, ax_arr):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(lab)\n",
    "            ax1.plot(xs, ys)\n",
    "\n",
    "    ax1.set_title(\"%s (Original)\" % (app[i]))\n",
    "    ax1.set_xlabel(\"Time (Normalized)\")\n",
    "    ax1.set_ylabel(\"Write I/O Intensity (Normalized)\")\n",
    "\n",
    "    ## full re-generation\n",
    "    try:\n",
    "        fname = \"flowmodel_%s_%d_DIM%d_app%d.torch\" % (dtype, nlen, MDIM, i)\n",
    "        flowmodel = NormalizingFlow(MDIM, 32)\n",
    "        flowmodel.load_state_dict(torch.load(fname))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    flowmodel.eval()\n",
    "    samples = (\n",
    "        (flowmodel.sample(random_normal_samples(NSAMPLES, dim=MDIM))).detach().numpy()\n",
    "    )\n",
    "\n",
    "    x = samples[:, :DIM]\n",
    "\n",
    "    xapp = np.zeros((len(x), len(app)), dtype=np.float32)\n",
    "    xapp[:, i] = 1.0\n",
    "    xx = np.hstack((x, xapp))\n",
    "\n",
    "    xx = torch.tensor(xx).to(device)\n",
    "\n",
    "    recon = model(xx)\n",
    "    if modelname == \"vae\":\n",
    "        recon = recon[0]\n",
    "    recon = recon.detach().cpu().numpy()\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 2)\n",
    "    for y in recon:\n",
    "        xs, ys = smooth(y)\n",
    "        ax3.plot(xs, ys)\n",
    "    ax3.set_title(\"%s (Generated)\" % (app[i]))\n",
    "    ax3.set_xlabel(\"Time (Normalized)\")\n",
    "    ax3.set_ylabel(\"Write I/O Intensity (Normalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_arr.shape, lab_arr.shape, ax_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 12\n",
    "for i in range(len(app)):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1, 1, 1)\n",
    "    idx = np.arange(len(recon_arr))\n",
    "    np.random.shuffle(idx)\n",
    "    for recon, lab, adx in zip(recon_arr[idx, :], lab_arr[idx, :], ax_arr[idx]):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(lab)\n",
    "            ax1.plot(xs, ys)\n",
    "\n",
    "    ax1.set_title(\"%s (Original)\" % (app[i]))\n",
    "    ax1.set_xlabel(\"Time (Normalized)\")\n",
    "    ax1.set_ylabel(\"Write I/O Intensity (Normalized)\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax2 = fig.add_subplot(1, 1, 1)\n",
    "    for recon, lab, adx in zip(recon_arr, lab_arr, ax_arr):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(recon)\n",
    "            ax2.plot(xs, ys)\n",
    "    ax2.set_title(\"%s (Generated)\" % (app[i]))\n",
    "    ax2.set_xlabel(\"Time (Normalized)\")\n",
    "    ax2.set_ylabel(\"Write I/O Intensity (Normalized)\")\n",
    "\n",
    "    ## full re-generation\n",
    "    try:\n",
    "        fname = \"flowmodel_%s_%d_DIM%d_app%d.torch\" % (dtype, nlen, MDIM, i)\n",
    "        flowmodel = NormalizingFlow(MDIM, 32)\n",
    "        flowmodel.load_state_dict(torch.load(fname))\n",
    "    except:\n",
    "        ## skip\n",
    "        continue\n",
    "\n",
    "    flowmodel.eval()\n",
    "    samples = (\n",
    "        (flowmodel.sample(random_normal_samples(NSAMPLES, dim=MDIM))).detach().numpy()\n",
    "    )\n",
    "\n",
    "    x = samples[:, :DIM]\n",
    "\n",
    "    xapp = np.zeros((len(x), len(app)), dtype=np.float32)\n",
    "    xapp[:, i] = 1.0\n",
    "    xx = np.hstack((x, xapp))\n",
    "\n",
    "    xx = torch.tensor(xx).to(device)\n",
    "\n",
    "    recon = model(xx)\n",
    "    if modelname == \"vae\":\n",
    "        recon = recon[0]\n",
    "    recon = recon.detach().cpu().numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax3 = fig.add_subplot(1, 1, 1)\n",
    "    for y in recon:\n",
    "        xs, ys = smooth(y)\n",
    "        ax3.plot(xs, ys)\n",
    "    ax3.set_title(\"%s (Generated)\" % (app[i]))\n",
    "    ax3.set_xlabel(\"Time (Normalized)\")\n",
    "    ax3.set_ylabel(\"Write I/O Intensity (Normalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowmodelname = \"flow\"  ## 'flow', 'gan', vae'\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "for i in range(len(app)):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=[12, 3])\n",
    "    # fig.subplots_adjust(left=0.1,right=.9,top=0.95,bottom=0.01,wspace=0.05,hspace=0.05)\n",
    "\n",
    "    idx = np.arange(len(recon_arr))\n",
    "    np.random.shuffle(idx)\n",
    "    for recon, lab, adx in zip(recon_arr[idx, :], lab_arr[idx, :], ax_arr[idx]):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(lab)\n",
    "            axs[0].plot(xs, ys, \".-\")\n",
    "\n",
    "    axs[0].set_title(\"%s\" % (app[i]), fontweight=\"bold\")\n",
    "\n",
    "    for recon, lab, adx in zip(recon_arr, lab_arr, ax_arr):\n",
    "        if adx == i:\n",
    "            xs, ys = smooth(recon)\n",
    "            axs[1].plot(xs, ys, \".-\")\n",
    "    axs[1].set_title(\"Generated (TG only)\")\n",
    "\n",
    "    ## full re-generation\n",
    "    fname = \"flowmodel_%s_%d_DIM%d_app%d_%s.torch\" % (\n",
    "        dtype,\n",
    "        nlen,\n",
    "        MDIM,\n",
    "        i,\n",
    "        flowmodelname,\n",
    "    )\n",
    "    print(\"Load \", fname)\n",
    "\n",
    "    try:\n",
    "        flowmodel = NormalizingFlow(MDIM, n_flows=10)\n",
    "        flowmodel.load_state_dict(torch.load(fname))\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        print(\"Unexpected error:\", e)\n",
    "        continue\n",
    "\n",
    "    flowmodel.eval()\n",
    "    samples = (\n",
    "        (flowmodel.sample(random_normal_samples(NSAMPLES, dim=MDIM))).detach().numpy()\n",
    "    )\n",
    "\n",
    "    x = samples[:, :DIM]\n",
    "\n",
    "    xapp = np.zeros((len(x), len(app)), dtype=np.float32)\n",
    "    xapp[:, i] = 1.0\n",
    "    xx = np.hstack((x, xapp))\n",
    "\n",
    "    xx = torch.tensor(xx).to(device)\n",
    "\n",
    "    recon = model(xx)\n",
    "    if modelname == \"vae\":\n",
    "        recon = recon[0]\n",
    "    recon = recon.detach().cpu().numpy()\n",
    "\n",
    "    for y in recon:\n",
    "        xs, ys = smooth(y)\n",
    "        axs[2].plot(xs, ys, \".-\")\n",
    "    axs[2].set_title(\"Generated (FG+TG)\")\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel=\"Time (Normalized)\", ylabel=\"\\nIntensity (Normalized)\")\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    # plt.gcf().text(0.01, 0.55, app[i], fontsize=16, weight='bold', rotation=90, ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('regen-%s-app%d-all.pdf'%(dtype, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
