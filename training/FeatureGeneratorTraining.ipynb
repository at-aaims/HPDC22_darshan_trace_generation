{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generator training: flow-based learning for Darshan trace\n",
    "\n",
    "This is for a demonstration of how to train Feture Generator for Darshan trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_block(torch.nn.Module):\n",
    "    def __init__(self, n, act=torch.nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        self.module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n, n), torch.nn.LeakyReLU(), torch.nn.Linear(n, n),\n",
    "        )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.module(inputs)\n",
    "        return self.act(x + inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, indim, outdim, nh=8, nz=4):\n",
    "        super(VAE, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.indim, nh)\n",
    "        self.fc21 = nn.Linear(nh, nz)\n",
    "        self.fc22 = nn.Linear(nh, nz)\n",
    "        self.fc3 = nn.Linear(nz, nh)\n",
    "        self.fc4 = nn.Linear(nh, self.outdim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.leaky_relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.leaky_relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.indim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def vae_loss_function(recon_x, x, mu, logvar, dim=20, alpha=1.0):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, dim), reduction=\"sum\")\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, dim), reduction=\"sum\")\n",
    "    # RMSE = torch.sum(RMSELoss(recon_x, x.view(-1, 20)))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + alpha * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/rmse-loss-function/16540/2\n",
    "def RMSELoss(yhat, y):\n",
    "    return torch.sqrt(torch.mean((yhat - y) ** 2) + torch.finfo(torch.float32).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, alpha=1.0):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 7), reduction=\"sum\")\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, 7), reduction=\"sum\")\n",
    "    # RMSE = torch.sum(RMSELoss(recon_x, x.view(-1, 20)))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + alpha * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, outdim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, indim, outdim=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(indim, 64),\n",
    "            ResNet_block(64),\n",
    "            ResNet_block(64),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            ResNet_block(32),\n",
    "            ResNet_block(32),\n",
    "            torch.nn.Linear(32, outdim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"FG training\")\n",
    "parser.add_argument(\n",
    "    \"--batch-size\",\n",
    "    type=int,\n",
    "    default=128,\n",
    "    metavar=\"N\",\n",
    "    help=\"input batch size for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of epochs to train (default: 10)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--log-interval\",\n",
    "    type=int,\n",
    "    default=1000,\n",
    "    metavar=\"N\",\n",
    "    help=\"how many batches to wait before logging training status\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args([\"--batch-size=32\"])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App list\n",
    "* Huge:\n",
    "    'PPP_Hierarchical.mpi.3d', 'cholla.paris-cuda', 'hacc_p3m',\n",
    "   'lbpm_random_force_simulator', 'main_parallel', 'nekrs',\n",
    "   'ramses3d', 'sigma.cplx.x', 'tusas', 'xgc-es-cpp-gpu'\n",
    "* Large:\n",
    "    'hf_summit_nvblas.x', 'hf_summit_oblate.x', 'lalibe',\n",
    "    'main_parallel', 'pmemd.cuda.MPI', 'prog_ccm_ex_summit_nat.exe',\n",
    "    'python', 's3d.x', 'xgc-es-cpp-gpu', 'xspecfem3D'\n",
    "* Medium:\n",
    "    'dirac.x', 'epw.x', 'lalibe', 'ngp', 'rmg-gpu', 's3d.x','xspecfem3D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "dtype = \"huge\"  #'huge', large', 'medium'\n",
    "APP = 2\n",
    "NSAMPLES = 100\n",
    "BATCH = 32\n",
    "n_flows = 10\n",
    "modelname = \"vae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dtype == \"huge\":\n",
    "    nlen, nclass, DIM = 26, 10, 75 + 7\n",
    "if dtype == \"large\":\n",
    "    nlen, nclass, DIM = 220, 10, 60 + 7\n",
    "if dtype == \"medium\":\n",
    "    # nlen, nclass = 945, 12\n",
    "    nlen, nclass, DIM = 417, 7, 25 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> Parameters\")\n",
    "for pname in [\"dtype\", \"DIM\", \"APP\", \"NSAMPLES\", \"BATCH\", \"n_flows\", \"modelname\"]:\n",
    "    print(\"%s: %r\" % (pname, eval(pname)))\n",
    "assert APP < nclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../data/train_x_%s_%d.npy\" % (dtype, nlen))\n",
    "y = np.load(\"../data/train_y_%s_%d.npy\" % (dtype, nlen))\n",
    "\n",
    "lb = np.load(\"../data/train_lb_%s_%d.npy\" % (dtype, nlen))\n",
    "app = np.load(\"../data/train_classes_%s_%d_%d.npy\" % (dtype, nlen, nclass))\n",
    "print(x.shape, y.shape, lb.shape, app.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.0001)\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "assert DIM == x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xapp = np.zeros((len(x), len(app)), dtype=np.float32)\n",
    "xapp[np.arange(len(x)), lb] = 1.0\n",
    "xapp.shape, x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(xapp)\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(x)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_density(x, p):\n",
    "    device = x.device\n",
    "    n, d = p.shape\n",
    "    assert len(x[0, :]) == d\n",
    "\n",
    "    y = torch.zeros_like(x).to(device)\n",
    "    for i in range(len(x)):\n",
    "        y[i, :] = (1 - torch.erf((p - x[i, :]) * 10).abs()).sum(axis=0) / n\n",
    "    return y\n",
    "\n",
    "\n",
    "def log_f_density(x, p):\n",
    "    n, d = p.shape\n",
    "    y = f_density(x, p)\n",
    "    return torch.log(y.sum(axis=1) / d + np.finfo(np.float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = list()\n",
    "for k in range(nclass):\n",
    "    s = 0\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    plt.figure(figsize=[12, 3])\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    for i, enc in enumerate(xapp):\n",
    "        if enc[k] == 1.0:\n",
    "            ax1.plot(x[i, :], \".-\", label=\"Original\")\n",
    "            ax2.plot(y[i, :], \".-\", label=\"Original\")\n",
    "            x_list.append(x[i, :])\n",
    "            y_list.append(y[i, :])\n",
    "            s += 1\n",
    "    px = np.array(x_list)\n",
    "    py = np.array(y_list)\n",
    "    p_list.append((px, py))\n",
    "    # print (px.shape)\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    plt.suptitle(\"%s (%d)\" % (app[k], s))\n",
    "\n",
    "[(x[0].shape, x[1].shape) for x in p_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lx = np.linspace(0, 1, int(1e2)+1)\n",
    "# lxx = np.vstack((lx,lx,lx)).T\n",
    "# print (lxx.shape)\n",
    "\n",
    "# z = (f_density(torch.Tensor(lxx), torch.Tensor(x_list)[:,4:7])).numpy()\n",
    "# print (z.shape)\n",
    "\n",
    "# plt.figure()\n",
    "# for i in range(len(z[0,:])):\n",
    "#     plt.plot(lx, z[:,i], label=i)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms = torch.Tensor(x_list)\n",
    "# n1 = D.Normal(ms[0,:], torch.ones(7)*0.01)\n",
    "# n2 = D.Normal(ms[1,:], torch.ones(7)*0.01)\n",
    "\n",
    "# plt.plot(ms[1,:], 's-', c='r');\n",
    "# samples = n2.sample([10]).detach().numpy()\n",
    "# plt.plot(samples.T, '-', c='b', alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "from normalizing_flows import NormalizingFlow\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils import random_normal_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px, py = p_list[APP]\n",
    "losses = []\n",
    "\n",
    "if modelname == \"flow\":\n",
    "    target_density = lambda z: log_f_density(z, torch.Tensor(px).to(z.device)[:, -DIM:])\n",
    "\n",
    "    model = NormalizingFlow(DIM, n_flows).to(device)\n",
    "\n",
    "    # RMSprop is what they used in renzende et al\n",
    "    optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=200)\n",
    "elif modelname == \"gan\":\n",
    "    loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "    model = Generator(1, DIM).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2_000, gamma=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", patience=200\n",
    "    )\n",
    "\n",
    "    discriminator = Discriminator(DIM, 1).to(device)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    scheduler_D = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_D, \"min\", patience=200\n",
    "    )\n",
    "    adversarial_loss = torch.nn.BCELoss()\n",
    "elif modelname == \"vae\":\n",
    "    model = VAE(1, DIM, nh=32, nz=16)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2_000, gamma=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", patience=2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.utils.data.TensorDataset(torch.tensor(px))\n",
    "\n",
    "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if args.cuda else {}\n",
    "sampler = torch.utils.data.RandomSampler(\n",
    "    training_data, replacement=True, num_samples=BATCH\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_data, batch_size=BATCH, drop_last=False, sampler=sampler, **kwargs\n",
    ")\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model params\")\n",
    "print(\"-\" * 20)\n",
    "num_params = 0\n",
    "for k, v in model.state_dict().items():\n",
    "    print(\"%20s\\t%20s\\t%10d\" % (k, list(v.shape), v.numel()))\n",
    "    num_params += v.numel()\n",
    "print(\"-\" * 50)\n",
    "print(\"%20s\\t%20s\\t%10d\" % (\"Total\", \"\", num_params))\n",
    "print(\"All (total, MB): %d %g\" % (num_params, num_params * 4 / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_ in range(5000):\n",
    "    model.train()\n",
    "    if iter_ % 100 == 0:\n",
    "        print(\"Iteration {}\".format(iter_))\n",
    "\n",
    "    if modelname == \"flow\":\n",
    "        samples = Variable(random_normal_samples(BATCH, dim=DIM)).to(device)\n",
    "\n",
    "        z_k, sum_log_det = model(samples * 0.01)\n",
    "        log_p_x = target_density(z_k)\n",
    "\n",
    "        # Reverse KL since we can evaluate target density but can't sample\n",
    "        loss = (-sum_log_det - (log_p_x)).mean()\n",
    "\n",
    "    elif modelname == \"gan\":\n",
    "        samples = Variable(random_normal_samples(BATCH, dim=1)).to(device)\n",
    "\n",
    "        (lab,) = next(iter(train_loader))\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        samples = Variable(random_normal_samples(BATCH, dim=1)).to(device)\n",
    "        recon_batch = model(samples)\n",
    "\n",
    "        valid = Variable(torch.Tensor(BATCH, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(torch.Tensor(BATCH, 1).fill_(0.0), requires_grad=False)\n",
    "        loss = adversarial_loss(discriminator(recon_batch), valid)\n",
    "    elif modelname == \"vae\":\n",
    "        samples = Variable(random_normal_samples(BATCH, dim=1)).to(device)\n",
    "\n",
    "        (lab,) = next(iter(train_loader))\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        recon_batch, mu, logvar = model(samples)\n",
    "        loss = vae_loss_function(recon_batch, lab, mu, logvar, dim=DIM)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if modelname == \"gan\":\n",
    "        ## Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(lab), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(recon_batch.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    if iter_ % 10 == 0:\n",
    "        print(\"Loss {} LR: {}\".format(loss.item(), optimizer.param_groups[0][\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the learning\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last loss:\", losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate a training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if modelname == \"flow\":\n",
    "    samples = Variable(random_normal_samples(NSAMPLES, dim=DIM)).to(device)\n",
    "    z_k = (model.sample(samples)).detach().cpu().numpy()\n",
    "elif modelname == \"gan\":\n",
    "    samples = Variable(random_normal_samples(NSAMPLES, dim=1)).to(device)\n",
    "    z_k = (model(samples)).detach().cpu().numpy()\n",
    "elif modelname == \"vae\":\n",
    "    samples = Variable(random_normal_samples(NSAMPLES, dim=1)).to(device)\n",
    "    recon_batch = model(samples)\n",
    "    z_k = recon_batch[0].detach().cpu().numpy()\n",
    "\n",
    "print(z_k.shape)\n",
    "plt.scatter(z_k[:, 0], z_k[:, 1], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    sns.jointplot(z_k[:, i], z_k[:, i + 1], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DIM):\n",
    "    plt.hist(z_k[:, i], bins=50, alpha=0.7, label=i)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for s in z_k:\n",
    "    plt.plot(s, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[lb == APP, :]\n",
    "plt.plot(xx.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "z = np.mean(z_k, axis=0)\n",
    "z_fit = (z - z.min()) / (z.max() - z.min())\n",
    "plt.plot(z_fit, label=\"gen\")\n",
    "plt.plot(np.mean(xx, axis=0), label=\"org\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.choice(range(len(py)), NSAMPLES)\n",
    "z_k_y = py[ids, :]\n",
    "\n",
    "print(z_k.shape, z_k_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"flowmodel_%s_%d_DIM%d_app%d_%s.torch\" % (dtype, nlen, DIM, APP, modelname)\n",
    "torch.save(model.state_dict(), fname)\n",
    "print(\"Model saved:\", fname)\n",
    "\n",
    "np.save(\"_train_x_%s_%d_DIM%d_app%d_%s.npy\" % (dtype, nlen, DIM, APP, modelname), z_k)\n",
    "np.save(\"_train_y_%s_%d_DIM%d_app%d_%s.npy\" % (dtype, nlen, DIM, APP, modelname), z_k_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = list()\n",
    "ly = list()\n",
    "lb_list = list()\n",
    "\n",
    "all_ready = True\n",
    "for APP in range(nclass):\n",
    "    try:\n",
    "        _x = np.load(\n",
    "            \"_train_x_%s_%d_DIM%d_app%d_%s.npy\" % (dtype, nlen, DIM, APP, modelname)\n",
    "        )\n",
    "        _y = np.load(\n",
    "            \"_train_y_%s_%d_DIM%d_app%d_%s.npy\" % (dtype, nlen, DIM, APP, modelname)\n",
    "        )\n",
    "\n",
    "        lx.append(_x)\n",
    "        ly.append(_y)\n",
    "        lb_list.append([APP,] * len(_x))\n",
    "    except:\n",
    "        all_ready &= False\n",
    "        print(\">>> Not ready:\", dtype, APP)\n",
    "        pass\n",
    "\n",
    "if all_ready:\n",
    "    xx = np.concatenate(lx)\n",
    "    yy = np.concatenate(ly)\n",
    "    ll = np.concatenate(lb_list)\n",
    "    print(\"FG:\", xx.shape, yy.shape, ll.shape)\n",
    "\n",
    "    np.save(\"flow_train_x_%s_%d_DIM%d_%s.npy\" % (dtype, nlen, DIM, modelname), xx)\n",
    "    np.save(\"flow_train_y_%s_%d_DIM%d_%s.npy\" % (dtype, nlen, DIM, modelname), yy)\n",
    "    np.save(\"flow_train_lb_%s_%d_DIM%d_%s.npy\" % (dtype, nlen, DIM, modelname), ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
